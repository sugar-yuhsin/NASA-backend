"""
æ”¹é€²çš„æ©Ÿå™¨å­¸ç¿’é æ¸¬è·¯ç”±
åŒ…å«å®Œæ•´çš„æ•¸æ“šå·¥ç¨‹æµç¨‹
"""

from fastapi import APIRouter, UploadFile, File, HTTPException
from typing import Dict, List, Any, Optional
import csv
import io
import os

router = APIRouter()

# æ¨¡å‹æª”æ¡ˆè·¯å¾‘
MODEL_PATH = "shark_rf_model_round_18.joblib"

# å…¨åŸŸæ¨¡å‹è®Šæ•¸
_model = None

def load_model():
    """è¼‰å…¥ joblib æ¨¡å‹"""
    global _model
    
    if _model is None:
        try:
            if not os.path.exists(MODEL_PATH):
                raise FileNotFoundError(f"æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {MODEL_PATH}")
            
            import joblib
            _model = joblib.load(MODEL_PATH)
            print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ: {type(_model).__name__}")
            return _model, None
            
        except ImportError as e:
            return None, f"ç¼ºå°‘å¿…è¦å¥—ä»¶: {e}"
        except Exception as e:
            return None, f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}"
    
    return _model, None

def data_engineering_pipeline(csv_content: str) -> tuple:
    """
    å®Œæ•´çš„æ•¸æ“šå·¥ç¨‹æµç¨‹
    1. è§£æ CSV
    2. ç‰¹å¾µé¸æ“‡
    3. æ•¸æ“šæ¸…ç†
    4. ç‰¹å¾µå·¥ç¨‹
    5. æ•¸æ“šæ¨™æº–åŒ–
    """
    try:
        # 1. è§£æ CSV
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        rows = list(csv_reader)
        
        if not rows:
            return None, None, None, "CSV æª”æ¡ˆæ²’æœ‰æ•¸æ“š"
        
        headers = list(rows[0].keys())
        print(f"ğŸ“‹ åŸå§‹æ¬„ä½: {headers}")
        
        # 2. ç‰¹å¾µé¸æ“‡ - é¸æ“‡å°é¯Šé­šé æ¸¬æœ‰ç”¨çš„ç‰¹å¾µ
        selected_features = [
            'Longitude', 'Latitude', 
            'SST_Value', 'SST_Gradient', 
            'CHL_Concentration', 'CHL_Gradient',
            'SSHA_Value', 'SSHA_Gradient',
            'Thermal_Front_Strength', 'Productivity_Index',
            'dist_to_eddy_center_km', 'Daily_Movement_km',
            'Days_To_Env_Data'
        ]
        
        # æª¢æŸ¥å“ªäº›ç‰¹å¾µå­˜åœ¨
        available_features = [f for f in selected_features if f in headers]
        print(f"ğŸ“Š å¯ç”¨ç‰¹å¾µ: {available_features}")
        
        # 3. æ•¸æ“šæ¸…ç†å’Œç‰¹å¾µå·¥ç¨‹
        processed_data = []
        labels = []  # å¦‚æœæœ‰æ¨™ç±¤çš„è©±
        
        for row in rows:
            # æå–ç‰¹å¾µ
            feature_vector = []
            
            for feature in available_features:
                value = row.get(feature, '0')
                
                # è™•ç†ç‰¹æ®Šå€¼
                if feature in ['is_in_eddy']:
                    # å¸ƒæ—å€¼è½‰æ•¸å€¼
                    feature_vector.append(1.0 if value.lower() in ['true', '1', 'yes'] else 0.0)
                elif feature == 'eddy_type':
                    # é¡åˆ¥è®Šæ•¸ç·¨ç¢¼
                    eddy_mapping = {'none': 0, 'anticyclonic': 1, 'cyclonic': 2}
                    feature_vector.append(float(eddy_mapping.get(value.lower(), 0)))
                else:
                    # æ•¸å€¼ç‰¹å¾µ
                    try:
                        numeric_value = float(value) if value != '' else 0.0
                        # è™•ç†ç•°å¸¸å€¼
                        if abs(numeric_value) > 1e6:  # è™•ç†æ¥µç«¯å€¼
                            numeric_value = 0.0
                        feature_vector.append(numeric_value)
                    except (ValueError, TypeError):
                        feature_vector.append(0.0)
            
            # ç‰¹å¾µå·¥ç¨‹ - å‰µå»ºæ–°ç‰¹å¾µ
            if len(feature_vector) >= 4:  # ç¢ºä¿æœ‰è¶³å¤ çš„åŸºæœ¬ç‰¹å¾µ
                # æº«åº¦èˆ‡è‘‰ç¶ ç´ çš„äº¤äº’ä½œç”¨
                sst_chl_interaction = feature_vector[2] * feature_vector[4] if len(feature_vector) > 4 else 0.0
                feature_vector.append(sst_chl_interaction)
                
                # è·é›¢ç‰¹å¾µï¼ˆå¾åŸé»çš„è·é›¢ï¼‰
                if len(feature_vector) >= 2:
                    distance_from_origin = (feature_vector[0]**2 + feature_vector[1]**2)**0.5
                    feature_vector.append(distance_from_origin)
            
            processed_data.append(feature_vector)
            
            # æå–æ¨™ç±¤ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'has_shark' in row:
                labels.append(int(float(row['has_shark'])))
        
        # 4. ç‰¹å¾µåç¨±
        final_feature_names = available_features.copy()
        if len(processed_data) > 0 and len(processed_data[0]) > len(available_features):
            final_feature_names.extend(['SST_CHL_Interaction', 'Distance_From_Origin'])
        
        # 5. æ•¸æ“šæ¨™æº–åŒ–ï¼ˆç°¡å–®ç‰ˆæœ¬ï¼‰
        if processed_data:
            import numpy as np
            data_array = np.array(processed_data)
            
            # åŸºæœ¬çµ±è¨ˆä¿¡æ¯
            means = np.mean(data_array, axis=0)
            stds = np.std(data_array, axis=0)
            
            # é¿å…é™¤ä»¥é›¶
            stds = np.where(stds == 0, 1, stds)
            
            # æ¨™æº–åŒ–
            normalized_data = (data_array - means) / stds
            
            return normalized_data.tolist(), final_feature_names, labels, None
        
        return processed_data, final_feature_names, labels, None
        
    except Exception as e:
        return None, None, None, f"æ•¸æ“šå·¥ç¨‹å¤±æ•—: {e}"

@router.post("/predict-advanced")
async def predict_with_advanced_preprocessing(file: UploadFile = File(...)):
    """
    ä¸Šå‚³ CSV æª”æ¡ˆï¼Œé€²è¡Œå®Œæ•´æ•¸æ“šå·¥ç¨‹å¾Œé æ¸¬
    
    åŒ…å«ï¼š
    - ç‰¹å¾µé¸æ“‡
    - æ•¸æ“šæ¸…ç†
    - ç‰¹å¾µå·¥ç¨‹
    - æ•¸æ“šæ¨™æº–åŒ–
    - æ¨¡å‹é æ¸¬
    """
    try:
        # é©—è­‰æª”æ¡ˆ
        if not file.filename or not file.filename.lower().endswith('.csv'):
            raise HTTPException(status_code=400, detail="åªæ”¯æ´ CSV æª”æ¡ˆ")
        
        # è¼‰å…¥æ¨¡å‹
        model, error = load_model()
        if error:
            raise HTTPException(status_code=500, detail=error)
        
        # è®€å–æª”æ¡ˆ
        contents = await file.read()
        csv_content = contents.decode('utf-8')
        
        # æ•¸æ“šå·¥ç¨‹æµç¨‹
        processed_data, feature_names, labels, process_error = data_engineering_pipeline(csv_content)
        if process_error:
            raise HTTPException(status_code=400, detail=process_error)
        
        # èª¿æ•´ç‰¹å¾µæ•¸é‡ä»¥åŒ¹é…æ¨¡å‹
        model_features = getattr(model, 'n_features_in_', 13)
        
        if len(processed_data[0]) > model_features:
            # å–å‰ N å€‹ç‰¹å¾µ
            processed_data = [row[:model_features] for row in processed_data]
            feature_names = feature_names[:model_features]
        elif len(processed_data[0]) < model_features:
            # è£œé›¶
            for i in range(len(processed_data)):
                while len(processed_data[i]) < model_features:
                    processed_data[i].append(0.0)
            while len(feature_names) < model_features:
                feature_names.append(f'feature_{len(feature_names)}')
        
        # é æ¸¬
        import numpy as np
        X = np.array(processed_data)
        predictions = model.predict(X)
        
        # å¦‚æœæ¨¡å‹æ”¯æ´ï¼Œç²å–é æ¸¬æ©Ÿç‡
        probabilities = None
        try:
            probabilities = model.predict_proba(X)
        except:
            pass
        
        # æº–å‚™çµæœ
        result = {
            "status": "success",
            "data_engineering": {
                "original_samples": len(processed_data),
                "features_used": len(feature_names),
                "feature_names": feature_names,
                "preprocessing_steps": [
                    "ç‰¹å¾µé¸æ“‡", "æ•¸æ“šæ¸…ç†", "ç‰¹å¾µå·¥ç¨‹", "æ•¸æ“šæ¨™æº–åŒ–"
                ]
            },
            "model_info": {
                "model_type": str(type(model).__name__),
                "expected_features": model_features,
                "model_classes": getattr(model, 'classes_', [0, 1]).tolist()
            },
            "predictions": {
                "values": predictions.tolist(),
                "count": len(predictions)
            }
        }
        
        # æ·»åŠ é æ¸¬æ©Ÿç‡
        if probabilities is not None:
            result["predictions"]["probabilities"] = probabilities.tolist()
            
            # é æ¸¬ä¿¡å¿ƒåº¦
            max_probs = np.max(probabilities, axis=1)
            result["predictions"]["confidence"] = {
                "mean": float(np.mean(max_probs)),
                "min": float(np.min(max_probs)),
                "max": float(np.max(max_probs))
            }
        
        # å¦‚æœæœ‰çœŸå¯¦æ¨™ç±¤ï¼Œè¨ˆç®—æº–ç¢ºåº¦
        if labels:
            accuracy = np.mean(predictions == np.array(labels))
            result["evaluation"] = {
                "accuracy": float(accuracy),
                "true_labels_available": True
            }
        
        # é æ¸¬çµ±è¨ˆ
        unique_preds, counts = np.unique(predictions, return_counts=True)
        result["predictions"]["distribution"] = dict(zip(unique_preds.tolist(), counts.tolist()))
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é æ¸¬å¤±æ•—: {str(e)}")

@router.post("/predict")
async def predict_with_csv(file: UploadFile = File(...)):
    """
    ç°¡å–®ç‰ˆæœ¬çš„é æ¸¬ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
    """
    # é‡å®šå‘åˆ°é«˜ç´šé æ¸¬
    return await predict_with_advanced_preprocessing(file)

@router.get("/model-info")
async def get_model_info():
    """ç²å–æ¨¡å‹è©³ç´°ä¿¡æ¯"""
    try:
        model, error = load_model()
        if error:
            return {"error": error}
        
        info = {
            "model_path": MODEL_PATH,
            "file_exists": os.path.exists(MODEL_PATH),
            "model_type": str(type(model).__name__),
            "file_size_mb": round(os.path.getsize(MODEL_PATH) / (1024 * 1024), 2)
        }
        
        # æ¨¡å‹å±¬æ€§
        attrs = ['n_features_in_', 'classes_', 'n_estimators', 'max_depth', 'feature_names_in_']
        for attr in attrs:
            if hasattr(model, attr):
                value = getattr(model, attr)
                if hasattr(value, 'tolist'):
                    value = value.tolist()
                info[attr] = value
        
        return info
        
    except Exception as e:
        return {"error": f"ç²å–æ¨¡å‹ä¿¡æ¯å¤±æ•—: {str(e)}"}

@router.get("/features")
async def get_feature_info():
    """ç²å–ç‰¹å¾µå·¥ç¨‹ä¿¡æ¯"""
    return {
        "selected_features": [
            "Longitude", "Latitude", 
            "SST_Value", "SST_Gradient", 
            "CHL_Concentration", "CHL_Gradient",
            "SSHA_Value", "SSHA_Gradient",
            "Thermal_Front_Strength", "Productivity_Index",
            "dist_to_eddy_center_km", "Daily_Movement_km",
            "Days_To_Env_Data"
        ],
        "engineered_features": [
            "SST_CHL_Interaction", "Distance_From_Origin"
        ],
        "preprocessing_steps": [
            "1. ç‰¹å¾µé¸æ“‡ - é¸æ“‡èˆ‡é¯Šé­šé æ¸¬ç›¸é—œçš„ç‰¹å¾µ",
            "2. æ•¸æ“šæ¸…ç† - è™•ç†ç¼ºå¤±å€¼å’Œç•°å¸¸å€¼",
            "3. ç‰¹å¾µå·¥ç¨‹ - å‰µå»ºäº¤äº’ç‰¹å¾µå’Œè·é›¢ç‰¹å¾µ",
            "4. æ•¸æ“šæ¨™æº–åŒ– - Z-score æ¨™æº–åŒ–"
        ]
    }