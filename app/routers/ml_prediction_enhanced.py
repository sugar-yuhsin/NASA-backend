"""
æ”¹é€²çš„æ©Ÿå™¨å­¸ç¿’é æ¸¬è·¯ç”±
åŒ…å«å®Œæ•´çš„æ•¸æ“šå·¥ç¨‹å’Œé è™•ç†æµç¨‹
"""

from fastapi import APIRouter, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from typing import Dict, List, Any, Optional
import io
import os
import numpy as np

router = APIRouter()

# æ¨¡å‹æª”æ¡ˆè·¯å¾‘
MODEL_PATH = "shark_rf_model_round_18.joblib"

# å…¨åŸŸæ¨¡å‹è®Šæ•¸
_model = None

def load_ml_model():
    """è¼‰å…¥ joblib æ¨¡å‹"""
    global _model
    
    if _model is None:
        try:
            import joblib
            
            if not os.path.exists(MODEL_PATH):
                return None, f"æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {MODEL_PATH}"
            
            _model = joblib.load(MODEL_PATH)
            return _model, "æ¨¡å‹è¼‰å…¥æˆåŠŸ"
            
        except ImportError:
            return None, "è«‹å®‰è£ joblib: pip install joblib"
        except Exception as e:
            return None, f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}"
    
    return _model, "æ¨¡å‹å·²è¼‰å…¥"

@router.post("/predict")
async def predict_with_csv_advanced(
    file: UploadFile = File(...),
    enable_augmentation: bool = False
):
    """
    ä¸Šå‚³ CSV æª”æ¡ˆä¸¦ä½¿ç”¨ RF æ¨¡å‹é€²è¡Œé æ¸¬ï¼ˆåŒ…å«å®Œæ•´æ•¸æ“šå·¥ç¨‹ï¼‰
    
    - **file**: ä¸Šå‚³çš„ CSV æª”æ¡ˆ
    - **enable_augmentation**: æ˜¯å¦å•Ÿç”¨æ•¸æ“šå¢å¼·ï¼ˆç”Ÿæˆè² æ¨£æœ¬ï¼‰
    
    è¿”å›é æ¸¬çµæœå’Œè©³ç´°çš„è™•ç†ä¿¡æ¯
    """
    try:
        # é©—è­‰æª”æ¡ˆé¡å‹
        if not file.filename.endswith('.csv'):
            raise HTTPException(
                status_code=400,
                detail="åªæ”¯æ´ CSV æª”æ¡ˆï¼Œè«‹ä¸Šå‚³ .csv æ ¼å¼çš„æª”æ¡ˆ"
            )
        
        # è¼‰å…¥æ¨¡å‹
        model, message = load_ml_model()
        if model is None:
            raise HTTPException(status_code=500, detail=message)
        
        # è®€å– CSV æª”æ¡ˆ
        contents = await file.read()
        csv_content = contents.decode('utf-8')
        
        print(f"ğŸ“ æ”¶åˆ° CSV æª”æ¡ˆ: {file.filename}")
        
        # ä½¿ç”¨æ”¹é€²çš„æ•¸æ“šè™•ç†æµç¨‹
        try:
            from data_processor import process_uploaded_csv
            
            features, feature_names, error = process_uploaded_csv(
                csv_content, 
                enable_augmentation=enable_augmentation
            )
            
            if error:
                raise HTTPException(status_code=400, detail=error)
            
            if features is None or len(features) == 0:
                raise HTTPException(status_code=400, detail="è™•ç†å¾Œæ²’æœ‰æœ‰æ•ˆçš„æ•¸æ“šè¡Œ")
            
            print(f"ğŸ”§ æ•¸æ“šé è™•ç†å®Œæˆ: {features.shape[0]} è¡Œ, {features.shape[1]} åˆ—")
            print(f"ğŸ“‹ ç‰¹å¾µ: {feature_names}")
            
        except ImportError:
            # å¦‚æœç„¡æ³•è¼‰å…¥æ•¸æ“šè™•ç†å™¨ï¼Œå›é€€åˆ°ç°¡å–®è™•ç†
            print("âš ï¸ ç„¡æ³•è¼‰å…¥æ•¸æ“šè™•ç†å™¨ï¼Œä½¿ç”¨ç°¡å–®è™•ç†æµç¨‹")
            features, feature_names, error = simple_process_csv(csv_content)
            if error:
                raise HTTPException(status_code=400, detail=error)
        
        # æª¢æŸ¥ç‰¹å¾µæ•¸é‡æ˜¯å¦åŒ¹é…æ¨¡å‹è¦æ±‚
        required_features = model.n_features_in_
        if features.shape[1] != required_features:
            print(f"âš ï¸ ç‰¹å¾µæ•¸é‡ä¸åŒ¹é…ï¼Œèª¿æ•´ç‚º {required_features} å€‹ç‰¹å¾µ")
            
            if features.shape[1] > required_features:
                # å¦‚æœç‰¹å¾µå¤ªå¤šï¼Œé¸æ“‡å‰ N å€‹
                features = features[:, :required_features]
                feature_names = feature_names[:required_features] if feature_names else None
            else:
                # å¦‚æœç‰¹å¾µä¸å¤ ï¼Œç”¨0å¡«å……
                padding = np.zeros((features.shape[0], required_features - features.shape[1]))
                features = np.hstack([features, padding])
                
                # æ›´æ–°ç‰¹å¾µåç¨±
                if feature_names:
                    for i in range(len(feature_names), required_features):
                        feature_names.append(f"feature_{i}")
        
        print(f"ğŸ”¢ æœ€çµ‚ç‰¹å¾µå½¢ç‹€: {features.shape}")
        
        # é€²è¡Œé æ¸¬
        try:
            predictions = model.predict(features)
            
            # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
            prediction_list = predictions.tolist()
            unique_predictions = list(set(prediction_list))
            
            result = {
                "status": "success",
                "file_info": {
                    "filename": file.filename,
                    "rows_processed": len(features),
                    "features_used": required_features,
                    "feature_names": feature_names[:required_features] if feature_names else None,
                    "data_augmentation_enabled": enable_augmentation
                },
                "model_info": {
                    "model_type": str(type(model).__name__),
                    "model_path": MODEL_PATH,
                    "required_features": required_features
                },
                "predictions": {
                    "values": prediction_list,
                    "count": len(prediction_list),
                    "unique_values": unique_predictions,
                    "unique_count": len(unique_predictions)
                }
            }
            
            # å¦‚æœæ˜¯äºŒåˆ†é¡å•é¡Œï¼Œæ·»åŠ è©³ç´°çµ±è¨ˆ
            if len(unique_predictions) <= 2:
                distribution = {}
                for pred in prediction_list:
                    distribution[pred] = distribution.get(pred, 0) + 1
                
                result["predictions"]["distribution"] = distribution
                
                # æ·»åŠ ç™¾åˆ†æ¯”
                total = len(prediction_list)
                percentages = {k: round(v/total*100, 2) for k, v in distribution.items()}
                result["predictions"]["percentages"] = percentages
            
            # å˜—è©¦ç²å–é æ¸¬æ©Ÿç‡
            try:
                probabilities = model.predict_proba(features)
                result["predictions"]["probabilities_summary"] = {
                    "shape": probabilities.shape,
                    "mean_confidence": float(np.mean(np.max(probabilities, axis=1))),
                    "min_confidence": float(np.min(np.max(probabilities, axis=1))),
                    "max_confidence": float(np.max(np.max(probabilities, axis=1)))
                }
                
                # åªå›å‚³å‰10å€‹æ¨£æœ¬çš„æ©Ÿç‡ï¼ˆé¿å…æ•¸æ“šå¤ªå¤§ï¼‰
                if len(probabilities) <= 10:
                    result["predictions"]["probabilities"] = probabilities.tolist()
                else:
                    result["predictions"]["probabilities_sample"] = probabilities[:10].tolist()
                    result["predictions"]["note"] = "åªé¡¯ç¤ºå‰10å€‹æ¨£æœ¬çš„é æ¸¬æ©Ÿç‡"
                
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•ç²å–é æ¸¬æ©Ÿç‡: {e}")
            
            return result
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"é æ¸¬å¤±æ•—: {e}")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è™•ç†å¤±æ•—: {str(e)}")

def simple_process_csv(csv_content: str) -> tuple:
    """ç°¡å–®çš„ CSV è™•ç†æµç¨‹ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
    try:
        import csv
        import io
        
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        data_rows = []
        columns = None
        
        for row in csv_reader:
            if columns is None:
                columns = list(row.keys())
            
            # è½‰æ›æ•¸å€¼é¡å‹
            processed_row = []
            for key, value in row.items():
                try:
                    # å˜—è©¦è½‰æ›ç‚ºæµ®é»æ•¸
                    processed_row.append(float(value))
                except (ValueError, TypeError):
                    # å¦‚æœç„¡æ³•è½‰æ›ï¼Œä½¿ç”¨ 0
                    processed_row.append(0.0)
            
            data_rows.append(processed_row)
        
        features = np.array(data_rows) if data_rows else None
        return features, columns, None
        
    except Exception as e:
        return None, None, f"CSV è™•ç†å¤±æ•—: {e}"

@router.get("/model-info")
async def get_model_info():
    """ç²å–æ¨¡å‹ä¿¡æ¯"""
    try:
        model, message = load_ml_model()
        
        info = {
            "model_path": MODEL_PATH,
            "file_exists": os.path.exists(MODEL_PATH),
            "status": "loaded" if model else "not_loaded",
            "message": message
        }
        
        if os.path.exists(MODEL_PATH):
            info["file_size_mb"] = round(os.path.getsize(MODEL_PATH) / (1024 * 1024), 2)
        
        if model:
            info["model_type"] = str(type(model).__name__)
            
            # å˜—è©¦ç²å–é¡å¤–ä¿¡æ¯
            try:
                if hasattr(model, 'n_features_in_'):
                    info['n_features'] = model.n_features_in_
                if hasattr(model, 'feature_names_in_'):
                    info['feature_names'] = model.feature_names_in_.tolist()
                if hasattr(model, 'classes_'):  
                    info['classes'] = model.classes_.tolist()
                if hasattr(model, 'n_estimators'):
                    info['n_estimators'] = model.n_estimators
                if hasattr(model, 'max_depth'):
                    info['max_depth'] = model.max_depth
            except:
                pass
        
        return info
        
    except Exception as e:
        return {"error": f"ç²å–æ¨¡å‹ä¿¡æ¯å¤±æ•—: {str(e)}"}

@router.get("/processing-info")
async def get_processing_info():
    """ç²å–æ•¸æ“šè™•ç†æµç¨‹ä¿¡æ¯"""
    try:
        from data_processor import OceanDataProcessor
        
        # å‰µå»ºä¸€å€‹ç¤ºä¾‹è™•ç†å™¨ä¾†ç²å–ä¿¡æ¯
        processor = OceanDataProcessor("")
        
        return {
            "status": "available",
            "features": processor.features,
            "feature_count": len(processor.features),
            "processing_steps": [
                "1. è¼‰å…¥æ•¸æ“š",
                "2. ç¯©é¸æ—¥æœŸç¯„åœ", 
                "3. ç¯©é¸ç‰¹å¾µ",
                "4. æ·»åŠ æ™‚é–“ç‰¹å¾µ (Day_of_Year, Month)",
                "5. å¡«è£œç¼ºå¤±å€¼ (ä¸­ä½æ•¸ç­–ç•¥)",
                "6. æ•¸æ“šå¢å¼· (å¯é¸) - ç”Ÿæˆè² æ¨£æœ¬",
                "7. æœ€çµ‚æ•¸æ“šæ¸…ç†"
            ],
            "augmentation_available": True,
            "description": "å®Œæ•´çš„æ•¸æ“šå·¥ç¨‹æµç¨‹ï¼ŒåŒ…å«ç‰¹å¾µå·¥ç¨‹ã€ç¼ºå¤±å€¼è™•ç†å’Œæ•¸æ“šå¢å¼·"
        }
        
    except ImportError:
        return {
            "status": "not_available",
            "message": "æ•¸æ“šè™•ç†å™¨æ¨¡çµ„ä¸å¯ç”¨",
            "fallback": "ä½¿ç”¨ç°¡å–®çš„æ•¸å€¼è½‰æ›è™•ç†"
        }