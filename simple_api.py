"""
ç°¡åŒ–ç‰ˆ FastAPI æµ·æ´‹æ•¸æ“š API
ä¸ä¾è³´è¤‡é›œçš„ Pydantic æ¨¡å‹å’Œèªè­‰ç³»çµ±
"""

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
import csv
from datetime import datetime, date
from typing import Dict, Optional, List, Any
import json
import io
import os

app = FastAPI(
    title="NASA æµ·æ´‹æ•¸æ“š API",
    description="ç°¡åŒ–ç‰ˆæµ·æ´‹æ•¸æ“šæŸ¥è©¢ API",
    version="1.0.0"
)

# è¨­å®š CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def parse_date(date_str: str) -> date:
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
    try:
        return datetime.strptime(date_str, '%Y-%m-%d').date()
    except:
        return None

def safe_float(value: str) -> Optional[float]:
    """å®‰å…¨è½‰æ›ç‚ºæµ®é»æ•¸"""
    try:
        if value == '' or value is None:
            return None
        return float(value)
    except:
        return None

def get_ocean_data_by_date(target_date: date) -> Dict:
    """æ ¹æ“šæ—¥æœŸç²å–æµ·æ´‹æ•¸æ“š"""
    csv_file = "comprehensive_shark_ocean_features - comprehensive_shark_ocean_features.csv"
    
    matching_records = []
    
    try:
        with open(csv_file, 'r', encoding='utf-8') as file:
            reader = csv.DictReader(file)
            
            for row in reader:
                row_date = parse_date(row['Date'])
                if row_date == target_date:
                    matching_records.append(row)
        
        if not matching_records:
            return {
                "date": str(target_date),
                "sst_value": None,
                "chl_value": None,
                "ssha_value": None,
                "data_count": 0,
                "message": "è©²æ—¥æœŸç„¡æ•¸æ“š"
            }
        
        # è¨ˆç®—å¹³å‡å€¼
        sst_values = [safe_float(record['SST_Value']) for record in matching_records]
        chl_values = [safe_float(record['CHL_Value']) for record in matching_records]
        ssha_values = [safe_float(record['SSHA_Value']) for record in matching_records]
        
        # éæ¿¾ None å€¼
        sst_values = [v for v in sst_values if v is not None]
        chl_values = [v for v in chl_values if v is not None]
        ssha_values = [v for v in ssha_values if v is not None]
        
        avg_sst = sum(sst_values) / len(sst_values) if sst_values else None
        avg_chl = sum(chl_values) / len(chl_values) if chl_values else None
        avg_ssha = sum(ssha_values) / len(ssha_values) if ssha_values else None
        
        return {
            "date": str(target_date),
            "sst_value": round(avg_sst, 6) if avg_sst is not None else None,
            "chl_value": round(avg_chl, 6) if avg_chl is not None else None,
            "ssha_value": round(avg_ssha, 6) if avg_ssha is not None else None,
            "data_count": len(matching_records),
            "message": "æŸ¥è©¢æˆåŠŸ"
        }
        
    except FileNotFoundError:
        return {
            "date": str(target_date),
            "sst_value": None,
            "chl_value": None,
            "ssha_value": None,
            "data_count": 0,
            "error": "CSV æª”æ¡ˆä¸å­˜åœ¨"
        }
    except Exception as e:
        return {
            "date": str(target_date),
            "sst_value": None,
            "chl_value": None,
            "ssha_value": None,
            "data_count": 0,
            "error": f"è®€å–æ•¸æ“šå¤±æ•—: {str(e)}"
        }

@app.get("/")
async def root():
    """æ ¹ç«¯é»"""
    return {
        "message": "ğŸŒŠ NASA æµ·æ´‹æ•¸æ“š API",
        "version": "1.0.0",
        "docs": "/docs",
        "example": "/ocean-data/2014-07-10"
    }

@app.get("/ocean-data/{date}")
async def get_ocean_data(date: str):
    """
    æ ¹æ“šæ—¥æœŸç²å–æµ·æ´‹æ•¸æ“š
    
    - **date**: æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD, ä¾‹å¦‚ 2014-07-10)
    
    è¿”å›è©²æ—¥æœŸçš„:
    - sst_value: æµ·è¡¨æº«åº¦å€¼
    - chl_value: è‘‰ç¶ ç´ å€¼  
    - ssha_value: æµ·é¢é«˜åº¦ç•°å¸¸å€¼
    """
    try:
        # è§£ææ—¥æœŸ
        query_date = datetime.strptime(date, '%Y-%m-%d').date()
        
        # ç²å–æ•¸æ“š
        result = get_ocean_data_by_date(query_date)
        
        if "error" in result:
            raise HTTPException(status_code=500, detail=result["error"])
        
        return result
        
    except ValueError:
        raise HTTPException(
            status_code=400, 
            detail="æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ YYYY-MM-DD æ ¼å¼ï¼Œä¾‹å¦‚: 2014-07-10"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¼ºæœå™¨éŒ¯èª¤: {str(e)}")

@app.post("/ocean-data")
async def get_ocean_data_post(request: dict):
    """
    é€šé POST è«‹æ±‚ç²å–æµ·æ´‹æ•¸æ“š
    
    è«‹æ±‚é«”ç¯„ä¾‹:
    ```json
    {
        "date": "2014-07-10"
    }
    ```
    """
    try:
        if "date" not in request:
            raise HTTPException(status_code=400, detail="è«‹æ±‚ä¸­ç¼ºå°‘ 'date' æ¬„ä½")
        
        # è§£ææ—¥æœŸ
        query_date = datetime.strptime(request["date"], '%Y-%m-%d').date()
        
        # ç²å–æ•¸æ“š
        result = get_ocean_data_by_date(query_date)
        
        if "error" in result:
            raise HTTPException(status_code=500, detail=result["error"])
        
        return result
        
    except ValueError:
        raise HTTPException(
            status_code=400, 
            detail="æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ YYYY-MM-DD æ ¼å¼ï¼Œä¾‹å¦‚: 2014-07-10"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¼ºæœå™¨éŒ¯èª¤: {str(e)}")

@app.get("/available-dates")
async def get_available_dates():
    """ç²å–å‰ 20 å€‹å¯ç”¨æ—¥æœŸ"""
    csv_file = "comprehensive_shark_ocean_features - comprehensive_shark_ocean_features.csv"
    dates = set()
    
    try:
        with open(csv_file, 'r', encoding='utf-8') as file:
            reader = csv.DictReader(file)
            
            for row in reader:
                if len(dates) >= 20:
                    break
                dates.add(row['Date'])
        
        return {
            "available_dates": sorted(list(dates)),
            "total_count": len(dates),
            "message": "å¯ç”¨æ—¥æœŸåˆ—è¡¨ (å‰20å€‹)"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è®€å–å¯ç”¨æ—¥æœŸå¤±æ•—: {str(e)}")

# ============================
# æ©Ÿå™¨å­¸ç¿’é æ¸¬åŠŸèƒ½
# ============================

# æ¨¡å‹æª”æ¡ˆè·¯å¾‘
MODEL_PATH = "shark_rf_model_round_18.joblib"

# å…¨åŸŸæ¨¡å‹è®Šæ•¸
_model = None

def load_ml_model():
    """è¼‰å…¥ joblib æ¨¡å‹"""
    global _model
    
    if _model is None:
        try:
            # æª¢æŸ¥æ˜¯å¦æœ‰ joblib
            import joblib
            
            if not os.path.exists(MODEL_PATH):
                return None, f"æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {MODEL_PATH}"
            
            _model = joblib.load(MODEL_PATH)
            return _model, "æ¨¡å‹è¼‰å…¥æˆåŠŸ"
            
        except ImportError:
            return None, "è«‹å®‰è£ joblib: pip install joblib"
        except Exception as e:
            return None, f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}"
    
    return _model, "æ¨¡å‹å·²è¼‰å…¥"

def process_csv_for_prediction(csv_content: str) -> tuple:
    """è™•ç† CSV å…§å®¹é€²è¡Œé æ¸¬"""
    try:
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        data_rows = []
        columns = None
        
        for row in csv_reader:
            if columns is None:
                columns = list(row.keys())
            
            # è½‰æ›æ•¸å€¼é¡å‹
            processed_row = []
            for key, value in row.items():
                try:
                    # å˜—è©¦è½‰æ›ç‚ºæµ®é»æ•¸
                    processed_row.append(float(value))
                except (ValueError, TypeError):
                    # å¦‚æœç„¡æ³•è½‰æ›ï¼Œä½¿ç”¨ 0
                    processed_row.append(0.0)
            
            data_rows.append(processed_row)
        
        return data_rows, columns, None
        
    except Exception as e:
        return None, None, f"CSV è™•ç†å¤±æ•—: {e}"

@app.post("/ml/predict")
async def predict_with_csv(file: UploadFile = File(...)):
    """
    ä¸Šå‚³ CSV æª”æ¡ˆä¸¦ä½¿ç”¨ RF æ¨¡å‹é€²è¡Œé æ¸¬
    
    - **file**: ä¸Šå‚³çš„ CSV æª”æ¡ˆ
    """
    try:
        # é©—è­‰æª”æ¡ˆé¡å‹
        if not file.filename.endswith('.csv'):
            raise HTTPException(
                status_code=400,
                detail="åªæ”¯æ´ CSV æª”æ¡ˆï¼Œè«‹ä¸Šå‚³ .csv æ ¼å¼çš„æª”æ¡ˆ"
            )
        
        # è¼‰å…¥æ¨¡å‹
        model, message = load_ml_model()
        if model is None:
            raise HTTPException(status_code=500, detail=message)
        
        # è®€å– CSV æª”æ¡ˆ
        contents = await file.read()
        csv_content = contents.decode('utf-8')
        
        # è™•ç† CSV æ•¸æ“š
        data_rows, columns, error = process_csv_for_prediction(csv_content)
        if error:
            raise HTTPException(status_code=400, detail=error)
        
        if not data_rows:
            raise HTTPException(status_code=400, detail="CSV æª”æ¡ˆæ²’æœ‰æœ‰æ•ˆçš„æ•¸æ“šè¡Œ")
        
        # é€²è¡Œé æ¸¬
        try:
            import numpy as np
            predictions = model.predict(np.array(data_rows))
            
            # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
            prediction_list = predictions.tolist()
            unique_predictions = list(set(prediction_list))
            
            result = {
                "status": "success",
                "file_info": {
                    "filename": file.filename,
                    "rows_processed": len(data_rows),
                    "columns": columns,
                    "column_count": len(columns) if columns else 0
                },
                "model_info": {
                    "model_type": str(type(model).__name__),
                    "model_path": MODEL_PATH
                },
                "predictions": {
                    "values": prediction_list,
                    "count": len(prediction_list),
                    "unique_values": unique_predictions,
                    "unique_count": len(unique_predictions)
                }
            }
            
            # å¦‚æœé æ¸¬å€¼ç¨®é¡å°‘ï¼Œé¡¯ç¤ºåˆ†ä½ˆ
            if len(unique_predictions) <= 10:
                distribution = {}
                for pred in prediction_list:
                    distribution[pred] = distribution.get(pred, 0) + 1
                result["predictions"]["distribution"] = distribution
            
            return result
            
        except ImportError:
            raise HTTPException(status_code=500, detail="è«‹å®‰è£ numpy: pip install numpy")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"é æ¸¬å¤±æ•—: {e}")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è™•ç†å¤±æ•—: {str(e)}")

@app.get("/ml/model-info")
async def get_model_info():
    """ç²å–æ¨¡å‹ä¿¡æ¯"""
    try:
        model, message = load_ml_model()
        
        info = {
            "model_path": MODEL_PATH,
            "file_exists": os.path.exists(MODEL_PATH),
            "status": "loaded" if model else "not_loaded",
            "message": message
        }
        
        if os.path.exists(MODEL_PATH):
            info["file_size_mb"] = round(os.path.getsize(MODEL_PATH) / (1024 * 1024), 2)
        
        if model:
            info["model_type"] = str(type(model).__name__)
            
            # å˜—è©¦ç²å–é¡å¤–ä¿¡æ¯
            try:
                if hasattr(model, 'n_features_in_'):
                    info['n_features'] = model.n_features_in_
                if hasattr(model, 'feature_names_in_'):
                    info['feature_names'] = model.feature_names_in_.tolist()
                if hasattr(model, 'classes_'):
                    info['classes'] = model.classes_.tolist()
                if hasattr(model, 'n_estimators'):
                    info['n_estimators'] = model.n_estimators
            except:
                pass
        
        return info
        
    except Exception as e:
        return {"error": f"ç²å–æ¨¡å‹ä¿¡æ¯å¤±æ•—: {str(e)}"}

@app.post("/ml/predict-simple")
async def predict_simple(data: Dict[str, Any]):
    """
    ç°¡å–®é æ¸¬æ¥å£ï¼Œæ¥å— JSON æ•¸æ“š
    
    - **data**: {"features": [[å€¼1, å€¼2, ...], [å€¼1, å€¼2, ...], ...]}
    """
    try:
        model, message = load_ml_model()
        if model is None:
            raise HTTPException(status_code=500, detail=message)
        
        if "features" not in data:
            raise HTTPException(status_code=400, detail="è«‹æä¾› 'features' æ¬„ä½")
        
        features = data["features"]
        if not isinstance(features, list) or not features:
            raise HTTPException(status_code=400, detail="features å¿…é ˆæ˜¯éç©ºçš„åˆ—è¡¨")
        
        try:
            import numpy as np
            predictions = model.predict(np.array(features))
            
            return {
                "status": "success",
                "predictions": predictions.tolist(),
                "count": len(predictions)
            }
            
        except ImportError:
            raise HTTPException(status_code=500, detail="è«‹å®‰è£ numpy: pip install numpy")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"é æ¸¬å¤±æ•—: {e}")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è™•ç†å¤±æ•—: {str(e)}")

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    return {"status": "healthy", "message": "API é‹è¡Œæ­£å¸¸"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)