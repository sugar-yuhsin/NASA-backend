"""
ML æ•¸æ“šé è™•ç†æ¨¡çµ„
åŒ…å«æ•¸æ“šæ¸…ç†ã€ç‰¹å¾µå·¥ç¨‹å’Œæ•¸æ“šå¢å¼·åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from math import radians, sin, cos, sqrt, atan2
from sklearn.impute import SimpleImputer
from scipy.interpolate import griddata
from typing import Tuple, List, Optional
import os

class OceanDataProcessor:
    """æµ·æ´‹æ•¸æ“šé è™•ç†å™¨"""
    
    def __init__(self, csv_file_path: str):
        self.csv_file_path = csv_file_path
        self.features = [
            'Longitude', 'Latitude', 'SST_Value', 'SST_Gradient', 
            'CHL_Concentration', 'CHL_Gradient', 'SSHA_Value', 
            'Thermal_Front_Strength', 'Productivity_Index', 
            'SSHA_Gradient', 'dist_to_eddy_center_km', 
            'Daily_Movement_km', 'Day_of_Year'
        ]
        self.imputer = None
        
    def load_data(self) -> pd.DataFrame:
        """è¼‰å…¥ CSV æ•¸æ“š"""
        try:
            df = pd.read_csv(self.csv_file_path)
            print(f"âœ… è¼‰å…¥æ•¸æ“šæˆåŠŸ: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
            return df
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ•¸æ“šå¤±æ•—: {e}")
            raise
    
    def filter_date_range(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç¯©é¸æ—¥æœŸç¯„åœ"""
        # è½‰æ›æ—¥æœŸæ ¼å¼
        df['Date'] = pd.to_datetime(df['Date'])
        
        # å¾æ•¸æ“šä¸­æ‰¾åˆ°æ—¥æœŸç¯„åœ
        start_date = df['Date'].min()
        end_date = df['Date'].max()
        
        print(f"ğŸ“… æ—¥æœŸç¯„åœ: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}")
        
        # ç¯©é¸æ—¥æœŸç¯„åœå…§çš„æ•¸æ“š
        filtered_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]
        
        print(f"ğŸ“Š ç¯©é¸å¾Œæ•¸æ“š: {filtered_df.shape[0]} è¡Œ")
        return filtered_df
    
    def add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æ™‚é–“ç‰¹å¾µ"""
        # ç¢ºä¿ Date æ˜¯ datetime é¡å‹
        df['Date'] = pd.to_datetime(df['Date'])
        
        # æ·»åŠ  Day_of_Year ç‰¹å¾µ
        df['Day_of_Year'] = df['Date'].dt.dayofyear
        df['Month'] = df['Date'].dt.month
        
        print(f"âœ… æ·»åŠ æ™‚é–“ç‰¹å¾µ: Day_of_Year, Month")
        return df
    
    def filter_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç¯©é¸éœ€è¦çš„ç‰¹å¾µ"""
        # æª¢æŸ¥å“ªäº›ç‰¹å¾µå­˜åœ¨æ–¼æ•¸æ“šä¸­
        available_features = []
        for feat in self.features:
            if feat in df.columns:
                available_features.append(feat)
            else:
                print(f"âš ï¸ ç‰¹å¾µ {feat} ä¸å­˜åœ¨æ–¼æ•¸æ“šä¸­")
        
        # ç¢ºä¿å¿…è¦çš„æ¬„ä½å­˜åœ¨
        required_cols = ['Date', 'Longitude', 'Latitude', 'has_shark']
        for col in required_cols:
            if col not in df.columns:
                print(f"âŒ å¿…è¦æ¬„ä½ {col} ä¸å­˜åœ¨")
                raise ValueError(f"å¿…è¦æ¬„ä½ {col} ä¸å­˜åœ¨")
        
        self.features = available_features
        print(f"ğŸ“‹ ä½¿ç”¨ç‰¹å¾µ: {len(self.features)} å€‹")
        print(f"   {self.features}")
        
        return df
    
    def impute_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¡«è£œç¼ºå¤±å€¼"""
        # ä½¿ç”¨ä¸­ä½æ•¸å¡«è£œæ•¸å€¼ç‰¹å¾µçš„ç¼ºå¤±å€¼
        self.imputer = SimpleImputer(strategy='median')
        
        # åªå°æ•¸å€¼ç‰¹å¾µé€²è¡Œå¡«è£œ
        numeric_features = [f for f in self.features if f in df.columns and df[f].dtype in ['float64', 'int64']]
        
        if numeric_features:
            print(f"ğŸ”§ å¡«è£œ {len(numeric_features)} å€‹æ•¸å€¼ç‰¹å¾µçš„ç¼ºå¤±å€¼")
            imputed_data = self.imputer.fit_transform(df[numeric_features])
            df[numeric_features] = imputed_data
        
        return df
    
    def haversine_distance(self, lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """è¨ˆç®—å…©é»é–“çš„è·é›¢ï¼ˆå…¬é‡Œï¼‰"""
        R = 6371  # åœ°çƒåŠå¾‘ï¼ˆå…¬é‡Œï¼‰
        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
        c = 2 * atan2(sqrt(a), sqrt(1-a))
        return R * c
    
    def augment_data(self, df: pd.DataFrame, grid_spacing: float = 0.09) -> pd.DataFrame:
        """æ•¸æ“šå¢å¼·ï¼šç”Ÿæˆç›¸éš”10å…¬é‡Œå…§ç„¡é¯Šé­šçš„é»"""
        print("ğŸ”„ é–‹å§‹æ•¸æ“šå¢å¼·...")
        
        # ç²å–æœ‰é¯Šé­šçš„åº§æ¨™
        shark_coords = df[df['has_shark'] == 1][['Longitude', 'Latitude']].values
        
        if len(shark_coords) == 0:
            print("âš ï¸ æ²’æœ‰ç™¼ç¾æœ‰é¯Šé­šçš„æ•¸æ“šé»")
            return df
        
        # ç²å–æ•¸æ“šç¯„åœ
        lon_min, lon_max = df['Longitude'].min(), df['Longitude'].max()
        lat_min, lat_max = df['Latitude'].min(), df['Latitude'].max()
        
        print(f"ğŸ“ æ•¸æ“šç¯„åœ: ç¶“åº¦ {lon_min:.2f} ~ {lon_max:.2f}, ç·¯åº¦ {lat_min:.2f} ~ {lat_max:.2f}")
        
        # ç”Ÿæˆç¶²æ ¼é»
        new_points = []
        total_points = 0
        valid_points = 0
        
        for lon in np.arange(lon_min, lon_max, grid_spacing):
            for lat in np.arange(lat_min, lat_max, grid_spacing):
                total_points += 1
                
                # æª¢æŸ¥æ˜¯å¦è·é›¢ä»»ä½•é¯Šé­šé»è¶…é10å…¬é‡Œ
                min_distance = min([
                    self.haversine_distance(lat, lon, shark_lat, shark_lon)
                    for shark_lon, shark_lat in shark_coords
                ])
                
                if min_distance > 10:  # è·é›¢æ‰€æœ‰é¯Šé­šé»éƒ½è¶…é10å…¬é‡Œ
                    new_points.append([lon, lat, 0])  # has_shark = 0
                    valid_points += 1
        
        print(f"ğŸ¯ ç”Ÿæˆ {valid_points}/{total_points} å€‹è² æ¨£æœ¬é»")
        
        if not new_points:
            print("âš ï¸ æ²’æœ‰ç”Ÿæˆæ–°çš„è² æ¨£æœ¬é»")
            return df
        
        # å‰µå»ºæ–°æ•¸æ“šæ¡†
        new_df = pd.DataFrame(new_points, columns=['Longitude', 'Latitude', 'has_shark'])
        
        # ç‚ºæ–°é»æ’å€¼å…¶ä»–ç‰¹å¾µ
        numeric_features = [f for f in self.features if f in df.columns and f not in ['Longitude', 'Latitude']]
        
        for feat in numeric_features:
            if feat in df.columns:
                try:
                    # ä½¿ç”¨ç·šæ€§æ’å€¼
                    interpolated_values = griddata(
                        (df['Longitude'].values, df['Latitude'].values), 
                        df[feat].values,
                        (new_df['Longitude'].values, new_df['Latitude'].values),
                        method='linear', 
                        fill_value=df[feat].median()
                    )
                    new_df[feat] = interpolated_values
                except Exception as e:
                    print(f"âš ï¸ ç‰¹å¾µ {feat} æ’å€¼å¤±æ•—ï¼Œä½¿ç”¨ä¸­ä½æ•¸å¡«å……: {e}")
                    new_df[feat] = df[feat].median()
        
        # è¨­ç½®æ—¥æœŸï¼ˆä½¿ç”¨åŸæ•¸æ“šçš„ä¸­ä½æ•¸æ—¥æœŸï¼‰
        median_date = df['Date'].median()
        new_df['Date'] = median_date
        new_df['Month'] = median_date.month
        new_df['Day_of_Year'] = median_date.timetuple().tm_yday
        
        # åˆä½µæ•¸æ“š
        augmented_df = pd.concat([df, new_df], ignore_index=True)
        
        print(f"âœ… æ•¸æ“šå¢å¼·å®Œæˆ: {df.shape[0]} â†’ {augmented_df.shape[0]} è¡Œ")
        return augmented_df
    
    def final_imputation(self, df: pd.DataFrame) -> pd.DataFrame:
        """æœ€çµ‚å¡«è£œè™•ç†"""
        print("ğŸ”§ é€²è¡Œæœ€çµ‚æ•¸æ“šæ¸…ç†...")
        
        # é‡æ–°å¡«è£œæ‰€æœ‰ç‰¹å¾µçš„ç¼ºå¤±å€¼
        numeric_features = [f for f in self.features if f in df.columns and df[f].dtype in ['float64', 'int64']]
        
        if numeric_features:
            final_imputer = SimpleImputer(strategy='median')
            imputed_data = final_imputer.fit_transform(df[numeric_features])
            df[numeric_features] = imputed_data
        
        return df
    
    def process_data(self, enable_augmentation: bool = True) -> pd.DataFrame:
        """å®Œæ•´çš„æ•¸æ“šè™•ç†æµç¨‹"""
        print("ğŸš€ é–‹å§‹æ•¸æ“šé è™•ç†...")
        
        # 1. è¼‰å…¥æ•¸æ“š
        df = self.load_data()
        
        # 2. ç¯©é¸æ—¥æœŸç¯„åœ
        df = self.filter_date_range(df)
        
        # 3. ç¯©é¸ç‰¹å¾µ
        df = self.filter_features(df)
        
        # 4. æ·»åŠ æ™‚é–“ç‰¹å¾µ
        df = self.add_time_features(df)
        
        # 5. å¡«è£œç¼ºå¤±å€¼
        df = self.impute_missing_values(df)
        
        # 6. æ•¸æ“šå¢å¼·ï¼ˆå¯é¸ï¼‰
        if enable_augmentation:
            df = self.augment_data(df)
        
        # 7. æœ€çµ‚å¡«è£œ
        df = self.final_imputation(df)
        
        print("âœ… æ•¸æ“šé è™•ç†å®Œæˆ!")
        print(f"ğŸ“Š æœ€çµ‚æ•¸æ“šå½¢ç‹€: {df.shape}")
        print(f"ğŸ¦ˆ é¯Šé­šæ¨£æœ¬: {(df['has_shark'] == 1).sum()} å€‹")
        print(f"ğŸŒŠ éé¯Šé­šæ¨£æœ¬: {(df['has_shark'] == 0).sum()} å€‹")
        
        return df
    
    def get_features_for_prediction(self, df: pd.DataFrame) -> np.ndarray:
        """ç²å–ç”¨æ–¼é æ¸¬çš„ç‰¹å¾µçŸ©é™£"""
        feature_cols = [f for f in self.features if f in df.columns]
        return df[feature_cols].values
    
    def get_labels(self, df: pd.DataFrame) -> np.ndarray:
        """ç²å–æ¨™ç±¤"""
        return df['has_shark'].values


def process_uploaded_csv(csv_content: str, enable_augmentation: bool = False) -> Tuple[np.ndarray, List[str], Optional[str]]:
    """
    è™•ç†ä¸Šå‚³çš„ CSV å…§å®¹
    
    Args:
        csv_content: CSV å…§å®¹å­—ç¬¦ä¸²
        enable_augmentation: æ˜¯å¦å•Ÿç”¨æ•¸æ“šå¢å¼·
    
    Returns:
        features: ç‰¹å¾µçŸ©é™£
        feature_names: ç‰¹å¾µåç¨±åˆ—è¡¨
        error: éŒ¯èª¤ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    """
    try:
        import io
        
        # å°‡å­—ç¬¦ä¸²è½‰æ›ç‚º DataFrame
        df = pd.read_csv(io.StringIO(csv_content))
        
        # å‰µå»ºè‡¨æ™‚è™•ç†å™¨
        processor = OceanDataProcessor("")
        processor.csv_file_path = None  # ä¸ä½¿ç”¨æ–‡ä»¶è·¯å¾„
        
        # æ‰‹å‹•è¨­ç½®æ•¸æ“š
        if 'Date' not in df.columns:
            # å¦‚æœæ²’æœ‰æ—¥æœŸæ¬„ä½ï¼Œæ·»åŠ ä¸€å€‹å‡çš„æ—¥æœŸ
            df['Date'] = pd.to_datetime('2024-01-01')
        
        if 'has_shark' not in df.columns:
            # å¦‚æœæ²’æœ‰é¯Šé­šæ¨™ç±¤ï¼Œæ·»åŠ ä¸€å€‹é è¨­å€¼
            df['has_shark'] = 0
        
        # ç¢ºä¿ Date æ˜¯ datetime é¡å‹
        df['Date'] = pd.to_datetime(df['Date'])
        
        # ç¯©é¸ç‰¹å¾µ
        df = processor.filter_features(df)
        
        # æ·»åŠ æ™‚é–“ç‰¹å¾µ
        df = processor.add_time_features(df)
        
        # å¡«è£œç¼ºå¤±å€¼
        df = processor.impute_missing_values(df)
        
        # æ•¸æ“šå¢å¼·ï¼ˆå¯é¸ï¼‰
        if enable_augmentation:
            df = processor.augment_data(df)
        
        # æœ€çµ‚å¡«è£œ
        df = processor.final_imputation(df)
        
        # ç²å–ç‰¹å¾µçŸ©é™£
        features = processor.get_features_for_prediction(df)
        feature_names = [f for f in processor.features if f in df.columns]
        
        return features, feature_names, None
        
    except Exception as e:
        return None, None, f"æ•¸æ“šè™•ç†å¤±æ•—: {str(e)}"


# æ¸¬è©¦å‡½æ•¸
def test_data_processing():
    """æ¸¬è©¦æ•¸æ“šè™•ç†æµç¨‹"""
    csv_file = "merged_shark_ocean_data.csv"
    
    if not os.path.exists(csv_file):
        print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {csv_file}")
        return
    
    processor = OceanDataProcessor(csv_file)
    
    try:
        # è™•ç†æ•¸æ“šï¼ˆä¸å•Ÿç”¨æ•¸æ“šå¢å¼·ä»¥ç¯€çœæ™‚é–“ï¼‰
        processed_df = processor.process_data(enable_augmentation=False)
        
        print(f"\nğŸ“‹ è™•ç†çµæœ:")
        print(f"   æ•¸æ“šå½¢ç‹€: {processed_df.shape}")
        print(f"   ç‰¹å¾µæ•¸é‡: {len(processor.features)}")
        print(f"   ç‰¹å¾µåˆ—è¡¨: {processor.features}")
        
        # æª¢æŸ¥æ¨™ç±¤åˆ†ä½ˆ
        label_counts = processed_df['has_shark'].value_counts()
        print(f"   æ¨™ç±¤åˆ†ä½ˆ: {dict(label_counts)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        return False


if __name__ == "__main__":
    test_data_processing()