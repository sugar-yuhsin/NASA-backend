#!/usr/bin/env python3
"""
åˆä½µé¯Šé­šæ•¸æ“šæª”æ¡ˆä¸¦æ·»åŠ æ¨™ç±¤
- v8.1_comprehensive_shark_features.csv: æœ‰é¯Šé­šæ•¸æ“š (label=1)
- v8.1_standardized_random_features.csv: ç„¡é¯Šé­šæ•¸æ“š (label=0)
"""

import pandas as pd
import os

def merge_shark_data():
    """åˆä½µå…©å€‹ CSV æª”æ¡ˆä¸¦æ·»åŠ é¯Šé­šæ¨™ç±¤"""
    
    # æª”æ¡ˆè·¯å¾‘
    shark_file = "v8.1_comprehensive_shark_features.csv"
    random_file = "v8.1_standardized_random_features.csv"
    output_file = "merged_shark_ocean_data.csv"
    
    print("ğŸ¦ˆ é–‹å§‹è™•ç†é¯Šé­šæ•¸æ“šåˆä½µ...")
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(shark_file):
        print(f"âŒ æ‰¾ä¸åˆ°é¯Šé­šæ•¸æ“šæª”æ¡ˆ: {shark_file}")
        return False
    
    if not os.path.exists(random_file):
        print(f"âŒ æ‰¾ä¸åˆ°éš¨æ©Ÿæ•¸æ“šæª”æ¡ˆ: {random_file}")
        return False
    
    try:
        # è®€å–é¯Šé­šæ•¸æ“š (æœ‰é¯Šé­š)
        print(f"ğŸ“Š è®€å–é¯Šé­šæ•¸æ“š: {shark_file}")
        shark_df = pd.read_csv(shark_file)
        shark_df['has_shark'] = 1  # æ·»åŠ é¯Šé­šæ¨™ç±¤
        print(f"   âœ… é¯Šé­šæ•¸æ“š: {len(shark_df)} è¡Œ")
        
        # è®€å–éš¨æ©Ÿæ•¸æ“š (ç„¡é¯Šé­š)
        print(f"ğŸ“Š è®€å–éš¨æ©Ÿæ•¸æ“š: {random_file}")
        random_df = pd.read_csv(random_file)
        random_df['has_shark'] = 0  # æ·»åŠ ç„¡é¯Šé­šæ¨™ç±¤
        print(f"   âœ… éš¨æ©Ÿæ•¸æ“š: {len(random_df)} è¡Œ")
        
        # åˆä½µæ•¸æ“š
        print("ğŸ”— åˆä½µæ•¸æ“š...")
        merged_df = pd.concat([shark_df, random_df], ignore_index=True)
        print(f"   âœ… åˆä½µå¾Œç¸½è¨ˆ: {len(merged_df)} è¡Œ")
        
        # æª¢æŸ¥æ•¸æ“šåˆ†ä½ˆ
        shark_count = merged_df['has_shark'].sum()
        no_shark_count = len(merged_df) - shark_count
        print(f"   ğŸ¦ˆ æœ‰é¯Šé­šæ•¸æ“š: {shark_count} è¡Œ")
        print(f"   ğŸŒŠ ç„¡é¯Šé­šæ•¸æ“š: {no_shark_count} è¡Œ")
        
        # ä¿å­˜åˆä½µå¾Œçš„æ•¸æ“š
        print(f"ğŸ’¾ ä¿å­˜åˆä½µæ•¸æ“šåˆ°: {output_file}")
        merged_df.to_csv(output_file, index=False)
        
        # é¡¯ç¤ºåˆä½µå¾Œçš„æ¬„ä½
        print(f"ğŸ“‹ åˆä½µå¾Œçš„æ¬„ä½ ({len(merged_df.columns)} å€‹):")
        for i, col in enumerate(merged_df.columns, 1):
            print(f"   {i:2d}. {col}")
        
        # é¡¯ç¤ºå‰å¹¾è¡Œç¤ºä¾‹
        print("\nğŸ“– åˆä½µå¾Œæ•¸æ“šç¤ºä¾‹ (å‰3è¡Œ):")
        print(merged_df.head(3).to_string())
        
        print(f"\nâœ… æ•¸æ“šåˆä½µå®Œæˆï¼è¼¸å‡ºæª”æ¡ˆ: {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ è™•ç†éç¨‹ä¸­å‡ºéŒ¯: {e}")
        return False

def verify_merged_data():
    """é©—è­‰åˆä½µå¾Œçš„æ•¸æ“š"""
    output_file = "merged_shark_ocean_data.csv"
    
    if not os.path.exists(output_file):
        print(f"âŒ æ‰¾ä¸åˆ°åˆä½µæª”æ¡ˆ: {output_file}")
        return False
    
    try:
        df = pd.read_csv(output_file)
        
        print("\nğŸ” æ•¸æ“šé©—è­‰:")
        print(f"   ç¸½è¡Œæ•¸: {len(df)}")
        print(f"   ç¸½æ¬„ä½æ•¸: {len(df.columns)}")
        print(f"   æœ‰é¯Šé­š (has_shark=1): {df['has_shark'].sum()} è¡Œ")
        print(f"   ç„¡é¯Šé­š (has_shark=0): {(df['has_shark'] == 0).sum()} è¡Œ")
        
        # æª¢æŸ¥æ—¥æœŸç¯„åœ
        df['Date'] = pd.to_datetime(df['Date'])
        print(f"   æ—¥æœŸç¯„åœ: {df['Date'].min()} åˆ° {df['Date'].max()}")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„æ—¥æœŸå’Œä½ç½®
        duplicates = df.duplicated(['Date', 'Longitude', 'Latitude']).sum()
        print(f"   é‡è¤‡è¨˜éŒ„: {duplicates} è¡Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ é©—è­‰éç¨‹ä¸­å‡ºéŒ¯: {e}")
        return False

if __name__ == "__main__":
    # åŸ·è¡Œåˆä½µ
    success = merge_shark_data()
    
    if success:
        # é©—è­‰åˆä½µçµæœ
        verify_merged_data()
    else:
        print("âŒ æ•¸æ“šåˆä½µå¤±æ•—")